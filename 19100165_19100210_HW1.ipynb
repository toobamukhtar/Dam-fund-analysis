{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fund Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be looking at the data released by the State Bank of Pakistan. The data is the daily bank-wise and donor-wise receipts of the fund for the Daimer Bhasha and Mohmand Dam. You can find them in the following link: http://www.sbp.org.pk/notifications/FD/DamFund/Damfund.htm\n",
    "\n",
    "We will see how to scrape data from the web, clean it, and import it into Pandas for data analysis purposes. There are however some things to note:\n",
    "1. As you can tell, the data is in PDF form. PDF is the most difficult to handle data format and if you get extremely broken CSV files, there isn't a need to worry, that's where the cleaning part comes in.\n",
    "2. We'll be using an API to convert the data from PDF to CSV, and then from CSV to Pandas. There are, however, other ways to do this. The reason we wanted to do this method is two-fold\n",
    "3. We will use regular-expressions to clean the data since the CSV's will be extremely inconsistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Getting the Data\n",
    "\n",
    "We will download the data from the link http://www.sbp.org.pk/notifications/FD/DamFund/Damfund.htm\n",
    "\n",
    "Questions to think about while going through the data:\n",
    "\n",
    "1. Are there any inconsistencies? \n",
    "2. Any particular values that pop out that would need to be taken care of later in the cleaning process?\n",
    "3. How many columns are there, each, in the PDF files? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Scraping              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using what is called the *requests* model to get an HTML page, and then use *BeautifulSoup* to parse that HTML page such that we are able to to derive the appropriate information from it. \n",
    "\n",
    "* [Requests Documentation](http://docs.python-requests.org/en/master/)\n",
    "* [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [BeautifulSoup + Requests Tutorial](https://www.pythonforbeginners.com/python-on-the-web/web-scraping-with-beautifulsoup)\n",
    "* [BeautifulSoup](https://medium.freecodecamp.org/how-to-scrape-websites-with-python-and-beautifulsoup-5946935d93fe) \n",
    "\n",
    "Above are the links for the web scraping libraries documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T12:44:41.051686Z",
     "start_time": "2018-10-09T12:44:39.971715Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import config\n",
    "import glob\n",
    "from pprint import pprint\n",
    "\n",
    "import csv\n",
    "import re # To use regular expressions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T12:44:41.131905Z",
     "start_time": "2018-10-09T12:44:41.104786Z"
    }
   },
   "outputs": [],
   "source": [
    "directory = 'SBF/'\n",
    "# os is being imported so you can make a new directory. \n",
    "if not os.path.exists('./' + directory):\n",
    "    os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T09:46:14.726818Z",
     "start_time": "2018-10-05T09:45:49.396761Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d70e9523c1478bbb5af98f4162ee6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "59 pdf files downloaded\n"
     ]
    }
   ],
   "source": [
    "# Open each PDF link\n",
    "# Save the PDF in a directory in the same folder \n",
    "    \n",
    "url = 'http://www.sbp.org.pk/notifications/FD/DamFund/Damfund.htm'\n",
    "r  = requests.get(url)\n",
    "# print(r.status_code)\n",
    "\n",
    "data = r.content\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "# print(soup.prettify())\n",
    "\n",
    "for links in tqdm_notebook(soup.find_all('a')):\n",
    "    href = links.get('href')\n",
    "    if href.endswith('pdf'):\n",
    "        pattern = re.findall(r'\\d{2}-\\d{2}-\\d{4}\\.\\w+', href)[0]\n",
    "        if not os.path.exists(directory + pattern):\n",
    "            # Skip downloading if file is already present\n",
    "            pdf_link = urljoin(url, href)\n",
    "            pdf_response = requests.get(pdf_link)\n",
    "            with open(directory + pattern, 'wb') as f:\n",
    "                f.write(pdf_response.content)\n",
    "        \n",
    "print(f'{len(os.listdir(directory))} pdf files downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Converting from PDF to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the Zamzar API with 2 accounts. We start jobs for all PDFs untill the credits are exhausted. We then download the CSVs when they're done. Now we cange the API key to point to the other acount and only post jobs for files which have not been downloaded yet. Due to the number of files we have in out dataset just 2 turns with teh Zamzar API are sufficient.  \n",
    "  \n",
    "*There was one file `27-08-2018.pdf` larger than 1MB which is the file size limit for free accounts on Zamzar. We used an online PDF compressor at `https://www.ilovepdf.com/compress_pdf` to bring the file size down to around 300KB. It was verified that the compressed file had no differences in text content.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T09:46:54.696800Z",
     "start_time": "2018-10-05T09:46:52.086434Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'credits_remaining': 28,\n",
      " 'plan': {'conversions_per_month': 100,\n",
      "          'maximum_file_size': 1048576,\n",
      "          'name': 'Test',\n",
      "          'price_per_month': 0},\n",
      " 'test_credits_remaining': 100}\n"
     ]
    }
   ],
   "source": [
    "# Zamzar API authentication\n",
    "api_key = '9e485cc256a330127609b10172392af7139683df'\n",
    "auth_token = HTTPBasicAuth(api_key, '')\n",
    "\n",
    "endpoint = \"https://api.zamzar.com/v1/account\"\n",
    "res = requests.get(endpoint, auth=auth_token)\n",
    "pprint(res.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T12:44:45.821658Z",
     "start_time": "2018-10-09T12:44:45.794734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 are already downloaded.\n"
     ]
    }
   ],
   "source": [
    "csv_directory = directory + 'csv/'\n",
    "already_done = set(f.partition('.')[0] for f in os.listdir(csv_directory)) \\\n",
    "               if os.path.exists(csv_directory) else set()\n",
    "print(f'{len(already_done)} are already downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T09:48:17.156098Z",
     "start_time": "2018-10-05T09:48:10.975984Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 are already downloaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a779d788a8e94cecabf856d284f897cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=58), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 new jobs posted\n"
     ]
    }
   ],
   "source": [
    "# You need a list to store all the job_ids from the response of posting the conversion job, \n",
    "# if you are using the Zamzar API\n",
    "job_ids = []\n",
    "\n",
    "for file_name in tqdm_notebook(glob.glob(directory + '*.pdf')):\n",
    "    ## Write code here to post a job, and append each job's id into job_ids ##\n",
    "    endpoint = \"https://api.zamzar.com/v1/jobs\"\n",
    "    with open(file_name, 'rb') as f:\n",
    "        just_name = file_name.partition('\\\\')[2].partition('.')[0]\n",
    "        if just_name in already_done:\n",
    "            continue\n",
    "        \n",
    "        file_content = {'source_file': f}\n",
    "        data_content = {'target_format': 'csv'}\n",
    "        res = requests.post(endpoint, \n",
    "                            data=data_content, \n",
    "                            files=file_content, \n",
    "                            auth=auth_token)\n",
    "        try:\n",
    "            job_ids.append(res.json()['id'])\n",
    "        except: \n",
    "            pprint(res.json())\n",
    "\n",
    "print(f'{len(job_ids)} new jobs posted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below this cell write the code to download the completed files. First check if a job_id's status is completed and wait until it is. After it has been completed, download the file and save it.\n",
    "\n",
    "The exact code required here is all in the documentation, the only additional task you have to do on your own is figure out a way to find out which file has just been received from the job_id, and name the local file.\n",
    "\n",
    "**Please look at the Example JSON response in the [documentation](https://developers.zamzar.com/docs) to learn how to figure out the filenames, job status etc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T12:44:48.841742Z",
     "start_time": "2018-10-09T12:44:48.814452Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(csv_directory):\n",
    "    os.mkdir(csv_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T09:51:48.674486Z",
     "start_time": "2018-10-05T09:51:42.964611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a220cc50e04e6c81923093a3223089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Job ID', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Downloading', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Your code goes here ##\n",
    "for job_id in tqdm_notebook(job_ids, desc='Job ID'):\n",
    "    endpoint = f\"https://api.zamzar.com/v1/jobs/{job_id}\"\n",
    "    res = requests.get(endpoint, auth=HTTPBasicAuth(api_key, ''))\n",
    "    \n",
    "    while res.json()['status'] != 'successful': \n",
    "        # Wait for job to be done, since jobs were posted in order\n",
    "        #   just waiting for the first job will be required. The rest \n",
    "        #   will probably be done by the time this loop exits\n",
    "        pass\n",
    "    \n",
    "    target_file = res.json()['target_files'][0]\n",
    "    file_id = target_file['id']\n",
    "    local_filename = csv_directory + target_file['name']\n",
    "\n",
    "    endpoint = f\"https://api.zamzar.com/v1/files/{file_id}/content\"\n",
    "    res = requests.get(endpoint, stream=True, auth=auth_token)\n",
    "\n",
    "    try:\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            file_iter = res.iter_content(chunk_size=1024)\n",
    "            for chunk in tqdm_notebook(file_iter, desc='Downloading', leave=False):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    f.flush()\n",
    "    except IOError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Parsing the CSV File\n",
    "\n",
    "We can not simply use Pandas read_csv here since the conversion is not perfect and there will be rows with different number of columns, which Pandas does not take care of.\n",
    "\n",
    "**The following steps are performed:**\n",
    "* Parsing CSV into pandas dataframe.\n",
    "* Each DataFrame should consist of three columns with headers Bank, Donor_Name, and Amount\n",
    "* The date should be retrieved from the given filename \n",
    "* The Donor_Name can be NaN, as it is in a lot of cases. \n",
    "* Remove all \"Page of\" rows\n",
    "* Don't include the header rows (e.g. \"SUPREME COURT FUND....\") into the DataFrame\n",
    "* The Amount should be converted into a Pandas numeric at the end\n",
    "* Use regex to extract information. Also use it to correctly format the 3 columns.\n",
    "* Use regex to extract the amount from the problem row and put the amount as the third column\n",
    "* Merge the rest of the string as a name of the donor in the 2nd column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T12:45:04.171570Z",
     "start_time": "2018-10-09T12:45:04.151240Z"
    }
   },
   "outputs": [],
   "source": [
    "def parser_all(filename):\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        orig = [row for row in reader]\n",
    "        \n",
    "        # Convert list into string\n",
    "        data = [' '.join(row) for row in orig[4:]]\n",
    "\n",
    "        # Clean commas in the currency amounts\n",
    "        data = [line.replace(',', '') for line in data]\n",
    "\n",
    "        # Remove 'Total' and 'Page number' rows\n",
    "        data = [row for row in data if 'Total' not in row and 'Page' not in row]\n",
    "\n",
    "        records = []\n",
    "        for line in data:\n",
    "            amount = re.search(r' [1-9]\\d*', line)\n",
    "            amount = amount.group() if amount else ''\n",
    "\n",
    "            bank = re.search(r'.*(LTD|Ltd|LIMITED|Limited|Bank|BANK|BSC|A/C)', line)\n",
    "            bank = bank.group() if bank else ''\n",
    "            \n",
    "            account_num = re.search(r'[^\\d]0\\d*', line)\n",
    "            account_num = account_num.group() if account_num else ''\n",
    "\n",
    "            person = line.replace(amount, '').replace(bank, '').replace(account_num, '')\n",
    "\n",
    "            records.append((bank.strip(), person.strip(), amount.strip(), line))\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T12:45:05.271666Z",
     "start_time": "2018-10-09T12:45:05.264039Z"
    }
   },
   "outputs": [],
   "source": [
    "example_file = csv_directory + '17-08-2018.csv' # Assuming the file is in the folder all_csvs and is named appropriately\n",
    "# This is one of the most problematic files which is why I have included this in the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T12:45:05.971591Z",
     "start_time": "2018-10-09T12:45:05.851698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bank</th>\n",
       "      <th>Donor_Name</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL BARAKA BANK (PAKISTAN) LTD</td>\n",
       "      <td>MOAZZAM ALI</td>\n",
       "      <td>200.0</td>\n",
       "      <td>AL BARAKA BANK (PAKISTAN) LTD MOAZZAM ALI 0117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL BARAKA BANK (PAKISTAN) LTD</td>\n",
       "      <td>MEHMOOD KHAN LODHI</td>\n",
       "      <td>500.0</td>\n",
       "      <td>AL BARAKA BANK (PAKISTAN) LTD MEHMOOD KHAN LOD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL BARAKA BANK (PAKISTAN) LTD</td>\n",
       "      <td>M UMAR</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>AL BARAKA BANK (PAKISTAN) LTD M UMAR 0117 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AL BARAKA BANK (PAKISTAN) LTD</td>\n",
       "      <td>SAFDAR</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>AL BARAKA BANK (PAKISTAN) LTD SAFDAR 0117 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL BARAKA BANK (PAKISTAN) LTD</td>\n",
       "      <td>AMBREEN ZIA</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>AL BARAKA BANK (PAKISTAN) LTD AMBREEN ZIA 0117...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Bank          Donor_Name  Amount  \\\n",
       "0  AL BARAKA BANK (PAKISTAN) LTD         MOAZZAM ALI   200.0   \n",
       "1  AL BARAKA BANK (PAKISTAN) LTD  MEHMOOD KHAN LODHI   500.0   \n",
       "2  AL BARAKA BANK (PAKISTAN) LTD              M UMAR  1000.0   \n",
       "3  AL BARAKA BANK (PAKISTAN) LTD              SAFDAR  1000.0   \n",
       "4  AL BARAKA BANK (PAKISTAN) LTD         AMBREEN ZIA  1600.0   \n",
       "\n",
       "                                                Line  \n",
       "0  AL BARAKA BANK (PAKISTAN) LTD MOAZZAM ALI 0117...  \n",
       "1  AL BARAKA BANK (PAKISTAN) LTD MEHMOOD KHAN LOD...  \n",
       "2     AL BARAKA BANK (PAKISTAN) LTD M UMAR 0117 1000  \n",
       "3     AL BARAKA BANK (PAKISTAN) LTD SAFDAR 0117 1000  \n",
       "4  AL BARAKA BANK (PAKISTAN) LTD AMBREEN ZIA 0117...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember, remove headers and convert all amounts to Numeric; if it can't be converted it needs to be NaN\n",
    "def read_csv(filename):\n",
    "    headers = ['Bank', 'Donor_Name', 'Amount', 'Line']\n",
    "    # ^ The entire line has been added to the DF to debug the cleaning process\n",
    "    \n",
    "    raw_data = parser_all(filename)\n",
    "    \n",
    "    df = pd.DataFrame.from_records(raw_data, columns=headers)\n",
    "    df['Amount'] = pd.to_numeric(df['Amount'])\n",
    "    \n",
    "    return df\n",
    "    \n",
    "read_csv(example_file).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Importing Full Dataset\n",
    "\n",
    "* Run the parser on all the files\n",
    "* For each file **add a 'Date' column, which should be inferred from the filename**\n",
    "* Concatenate each DataFrame into one large DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T12:45:17.291590Z",
     "start_time": "2018-10-09T12:45:09.101503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9999f312e5144975ab5e0315310dbda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=58), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            Bank        Donor_Name  Amount        Date  \\\n",
      "0  AL BARAKA BANK (PAKISTAN) LTD       DAM UL HUDA  2000.0  2018-08-01   \n",
      "1  AL BARAKA BANK (PAKISTAN) LTD       FARIS AHMED  2000.0  2018-08-01   \n",
      "2  AL BARAKA BANK (PAKISTAN) LTD    MUHAMMAD AJMAL  1000.0  2018-08-01   \n",
      "3  AL BARAKA BANK (PAKISTAN) LTD  SHAHEENA SULTANA  1000.0  2018-08-01   \n",
      "4  AL BARAKA BANK (PAKISTAN) LTD       RASHID KHAN  1000.0  2018-08-01   \n",
      "\n",
      "                                                Line  \n",
      "0  AL BARAKA BANK (PAKISTAN) LTD DAM UL HUDA 2000.00  \n",
      "1  AL BARAKA BANK (PAKISTAN) LTD FARIS AHMED 2000.00  \n",
      "2  AL BARAKA BANK (PAKISTAN) LTD MUHAMMAD AJMAL 1...  \n",
      "3  AL BARAKA BANK (PAKISTAN) LTD SHAHEENA SULTANA...  \n",
      "4  AL BARAKA BANK (PAKISTAN) LTD RASHID KHAN 1000.00  \n",
      "(174635, 5)\n",
      "                            Bank       Donor_Name  Amount        Date  \\\n",
      "174630  Zarai Taraqiati Bank Ltd           M KHAN   500.0  2018-08-31   \n",
      "174631  Zarai Taraqiati Bank Ltd  INAYAT ALI KHAN   500.0  2018-08-31   \n",
      "174632  Zarai Taraqiati Bank Ltd         M HAYYAT   800.0  2018-08-31   \n",
      "174633  Zarai Taraqiati Bank Ltd    SYED RABNAWAZ  1000.0  2018-08-31   \n",
      "174634  Zarai Taraqiati Bank Ltd          M USMAN  1300.0  2018-08-31   \n",
      "\n",
      "                                                Line  \n",
      "174630           Zarai Taraqiati Bank Ltd M KHAN 500  \n",
      "174631  Zarai Taraqiati Bank Ltd INAYAT ALI KHAN 500  \n",
      "174632         Zarai Taraqiati Bank Ltd M HAYYAT 800  \n",
      "174633   Zarai Taraqiati Bank Ltd SYED RABNAWAZ 1000  \n",
      "174634         Zarai Taraqiati Bank Ltd M USMAN 1300  \n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(csv_directory + '*.csv')\n",
    "\n",
    "full_data = pd.DataFrame()\n",
    "for file in tqdm_notebook(files):\n",
    "    date = re.search(r'\\d{2}-\\d{2}-\\d{4}', file)\n",
    "    date = date.group()\n",
    "    df = read_csv(file)  \n",
    "    # Final Date format is %y %m %d\n",
    "    df['Date'] = datetime.strptime(date, '%d-%m-%Y').date()\n",
    "    df = df[['Bank', 'Donor_Name', 'Amount', 'Date', 'Line']]\n",
    "    full_data = full_data.append(df)\n",
    "full_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(full_data.head())\n",
    "print(full_data.shape)\n",
    "print(full_data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Integrity Checks\n",
    "\n",
    "**Questions to ask?**\n",
    "* How many NaN values are there in each column? Why are they there? \n",
    "* What are the maximum and minimum values, is there anything peculiar about the max values?\n",
    "* Are there any rows which are not NaN but should still be a different DataFrame altogether?\n",
    "* Should these problem rows be removed? Can they be useful in other ways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T12:10:17.685822Z",
     "start_time": "2018-10-09T12:10:17.415582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Strings:\n",
      "Donor_name: 3134\n",
      "Bank: 6562\n",
      "Amount: 0\n"
     ]
    }
   ],
   "source": [
    "# Using the following operation we can see that the rows where no Bank is present\n",
    "# are the records in the data with missing Bank information. \n",
    "# People only wrote their name and address\n",
    "full_data[full_data['Bank'] == '']\n",
    "\n",
    "is_empty_string = lambda x: x == ''\n",
    "empty_donor = np.where(full_data['Donor_Name'].apply(is_empty_string))\n",
    "empty_bank = np.where(full_data['Bank'].apply(is_empty_string))\n",
    "empty_amount = np.where(full_data['Amount'].apply(is_empty_string))\n",
    "\n",
    "print('Empty Strings:')\n",
    "print(f'Donor_name: {len(empty_donor[0])}\\nBank: {len(empty_bank[0])}\\nAmount: {len(empty_amount[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T12:06:21.877383Z",
     "start_time": "2018-10-09T12:06:21.741007Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bank             0\n",
       "Donor_Name       0\n",
       "Amount        6449\n",
       "Date             0\n",
       "Line             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the following operation we can see that the rows where no Amount is present\n",
    "# are the records in the data with missing currency information. \n",
    "full_data[full_data['Amount'].isna()]\n",
    "\n",
    "# Includes Nan,None. Does not include empty strings\n",
    "full_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T11:57:45.560541Z",
     "start_time": "2018-10-09T11:57:45.383424Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bank             0\n",
       "Donor_Name       0\n",
       "Amount        6449\n",
       "Date             0\n",
       "Line             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the following operation we can see that the rows where no Amount is present\n",
    "# are the records in the data with missing currency information. \n",
    "full_data[full_data['Amount'].isnull()]\n",
    "\n",
    "full_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T12:46:36.000687Z",
     "start_time": "2018-10-09T12:46:35.664562Z"
    },
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bank                    \n",
       "Donor_Name              \n",
       "Amount                 1\n",
       "Date          2018-07-06\n",
       "Line                1000\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column-wise independant max\n",
    "full_data.min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T12:46:54.720697Z",
     "start_time": "2018-10-09T12:46:54.444362Z"
    },
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bank           salah ud din FIRST WOMEN BANK LTD\n",
       "Donor_Name                                zumain\n",
       "Amount                               3.74056e+17\n",
       "Date                                  2018-10-04\n",
       "Line          donations received from Muhamma   \n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column-wise independant max\n",
    "full_data.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T12:47:46.640412Z",
     "start_time": "2018-10-09T12:47:46.600475Z"
    },
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bank                                      Allied Bank Limited\n",
       "Donor_Name                             H NO265 A F MIRPUR 200\n",
       "Amount                                                      1\n",
       "Date                                               2018-08-01\n",
       "Line          Allied Bank Limited H NO265 A F 1 MIRPUR 200.00\n",
       "Name: 140, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The row with min Amount\n",
    "full_data.loc[full_data['Amount'].idxmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T12:47:53.140183Z",
     "start_time": "2018-10-09T12:47:53.123034Z"
    },
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bank                                         Habib Bank Limited\n",
       "Donor_Name           KASHIF ALI MAKHDOMI ARL MORGAH 03215384822\n",
       "Amount                                              3.74056e+17\n",
       "Date                                                 2018-09-26\n",
       "Line          Habib Bank Limited KASHIF ALI MAKHDOMI ARL MOR...\n",
       "Name: 148170, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The row with max Amount\n",
    "full_data.loc[full_data['Amount'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows containing the total amount for the corresponding banks should either be kept in a different dataframe or calculated using a grouping operation. We have already removed these rows from our dataset.\n",
    "\n",
    "Another set of problematic rows are the donations made using SMS. Since they do not have a bank associated with them directly, the `Bank` column will be empty for these rows. It would be better to keep these donations in a separate dataframe. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
