{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework we'll be looking at the data released by the State Bank of Pakistan. The data is the daily bank-wise and donor-wise receipts of the fund for the Daimer Bhasha and Mohmand Dam. You can find them in the following link: http://www.sbp.org.pk/notifications/FD/DamFund/Damfund.htm. Take a moment to look around the data and try to figure out what the possible challenges could be.\n",
    "\n",
    "The main purpose of this homework is to teach how to scrape data from the web, clean it, and import it into Pandas for data analysis purposes. There are however some things to note:\n",
    "1. As you can tell, the data is in PDF form. PDF is the most difficult to handle data format and if you get extremely broken CSV files, there isn't a need to worry, that's where the cleaning part comes in.\n",
    "2. We'll be using an API to convert the data from PDF to CSV, and then from CSV to Pandas. There are, however, other ways to do this. The reason we wanted to do this method is two-fold\n",
    "    * It will teach you how to communicate with APIs using Python, which will be a useful skill when you want to deploy your data models as an API so that it can work with other APIs that need those data models. Moreover, a lot of data you get in the real world is from APIs. \n",
    "    * The CSV will be extremely inconsistent, so it will give you immense practice with using regular-expressions, which are extremely important in the Data Science tool-kit.\n",
    "    \n",
    "Submit the notebooks in a similar format to the Labs: print the relevant output in each cell **only if it has an output. The initial scraping and converting does not have any output**, and name the notebooks as:\n",
    "**rollnumber_HW1.ipynb** for e.g **20100237_H1.ipynb**\n",
    "\n",
    "Please make sure you complete full parts (denoted by a Header each in this notebook) as the grading will be based on parts. Needless to say, do not copy someone else's code. In most Data Science careers, the main skill is not how good you are at coding, but how well you are able to use the tools at your disposal and what inferences you are able to make with the information that you have. Thus, while you might be able to do the HW by looking at someone else's code, unless you go through the actual thought process, you won't learn a lot.\n",
    "\n",
    "We'll be using a lot of libraries in this tutorial, make sure you go through them so you understand what they are used for.\n",
    "\n",
    "**NOTE: If you are more comfortable doing so, as I am, you can do the assignment on your preferred text editor on simple Python and then write the code neatly in a notebook.** Personally, I find Sublime/Vim easier to use than Jupyter, mostly since a lot of shortcuts there make coding much easier, while here the shortcuts are more about navigation and controlling your cells.\n",
    "\n",
    "**The homework is to be done in pairs of 2.** \n",
    "\n",
    "**Naming convention: rollnumber1_rollnumber2_HW1.ipynb**\n",
    "\n",
    "Total Marks: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Getting the Data\n",
    "\n",
    "You can have a look at the data through the link given above. Download a few PDF files and go through the data to see what it looks like. How many columns are there, each, in the PDF files? Are there any inconsistencies? Any particular values that pop out that would need to be taken care of later in your cleaning? Think of all these questions when going through the initial PDF because they will prove really helpful when you can not figure out why there are so many \"NaN\" values in your final DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Scraping              \n",
    "Marks: 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using what is called the *requests* model to get an HTML page, and then use *BeautifulSoup* to parse that HTML page such that we are able to to derive the appropriate information from it. I recommend you go through the documentation of each to learn more about how to use the libraries. \n",
    "\n",
    "* [Requests Documentation](http://docs.python-requests.org/en/master/)\n",
    "* [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [BeautifulSoup + Requests Tutorial](https://www.pythonforbeginners.com/python-on-the-web/web-scraping-with-beautifulsoup)\n",
    "* [BeautifulSoup](https://medium.freecodecamp.org/how-to-scrape-websites-with-python-and-beautifulsoup-5946935d93fe) Note that this tutorial is more detailed. I would highly recommend you go through this as well even though the library used here is urllib2 instead of requests (which you can do as well!). It also links to more web-scraping libraries like Scrapy for more complicated scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T09:45:46.350118Z",
     "start_time": "2018-10-05T09:45:43.797228Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "directory = 'SBF/'\n",
    "# os is being imported so you can make a new directory. \n",
    "if not os.path.exists('./' + directory):\n",
    "    os.mkdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T09:46:14.726818Z",
     "start_time": "2018-10-05T09:45:49.396761Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d70e9523c1478bbb5af98f4162ee6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "59 pdf files downloaded\n"
     ]
    }
   ],
   "source": [
    "## Write code here that will:\n",
    "    # Open each PDF link\n",
    "    # Save the PDF in a directory in the same folder \n",
    "    \n",
    "    \n",
    "# Your code goes here #\n",
    "url = 'http://www.sbp.org.pk/notifications/FD/DamFund/Damfund.htm'\n",
    "r  = requests.get(url)\n",
    "# print(r.status_code)\n",
    "\n",
    "data = r.content\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "# print(soup.prettify())\n",
    "\n",
    "for links in tqdm_notebook(soup.find_all('a')):\n",
    "    href = links.get('href')\n",
    "    if href.endswith('pdf'):\n",
    "        pattern = re.findall(r'\\d{2}-\\d{2}-\\d{4}\\.\\w+', href)[0]\n",
    "        if not os.path.exists(directory + pattern):\n",
    "            # Skip downloading if file is already present\n",
    "            pdf_link = urljoin(url, href)\n",
    "            pdf_response = requests.get(pdf_link)\n",
    "            with open(directory + pattern, 'wb') as f:\n",
    "                f.write(pdf_response.content)\n",
    "        \n",
    "print(f'{len(os.listdir(directory))} pdf files downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Converting from PDF to CSV\n",
    "Marks: 15\n",
    "\n",
    "You have two possible options between deciding what API to use for the conversion task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first option is communicating with an API called [Zamzar](https://www.zamzar.com) to send each PDF, ask them to convert it into CSV, and then download the converted CSV. They provide sample code to do everything from generating a simple request to starting a conversion job, checking for completion, and then downloading the finished file. You can find this information on the [Zamzar Documentation](https://developers.zamzar.com/docs) page.\n",
    "\n",
    "**Important Information: **\n",
    "\n",
    "The API only provides 100 points of free conversion, and each PDF to CSV conversion costs 3 points, that means with one account you can only convert **33** PDFs. However, this also means you have very little room to play around with this API, unless you have an extra email-address, so you need to be very careful when coding to communicate with this API. \n",
    "\n",
    "Moreover, the API only keeps the converted files for one day with a free account, so make sure you do this part in one go.\n",
    "\n",
    "**Note: Using the Zamzar API grants a bonus of 10 marks. This will help if you are not able to complete this assignment, or it can be used up in a later assignment if you get 110/100 marks in this one.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another extremely simple API is the [PDF Tables](https://pdftables.com) API which is much simpler to use than the Zamzar API, however does not allow you to check the job for completion or for any intermediate steps. Moreover, this requires the installation of a library. Once again, they allow only 50 versions for free, but that is enough conversions for us. This [blog post](https://pdftables.com/blog/pdf-to-excel-with-python) will help you figure out how to convert the PDF to CSV using Python.\n",
    "\n",
    "The cons of this API is that it will not really teach you any proper API communcation through requests since you do not have to navigate through any requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T09:46:28.606570Z",
     "start_time": "2018-10-05T09:46:28.386750Z"
    }
   },
   "outputs": [],
   "source": [
    "from requests.auth import HTTPBasicAuth\n",
    "import config\n",
    "import glob\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the Zamzar API with 2 accounts. We start jobs for all PDFs untill the credits are exhausted. We then download the CSVs when they're done. Now we cange the API key to point to the other acount and only post jobs for files which have not been downloaded yet. Due to the number of files we have in out dataset just 2 turns with teh Zamzar API are sufficient.  \n",
    "  \n",
    "*There was one file `27-08-2018.pdf` larger than 1MB which is the file size limit for free accounts on Zamzar. We used an online PDF compressor at `https://www.ilovepdf.com/compress_pdf` to bring the file size down to around 300KB. It was verified that the compressed file had no differences in text content.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T09:46:54.696800Z",
     "start_time": "2018-10-05T09:46:52.086434Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'credits_remaining': 28,\n",
      " 'plan': {'conversions_per_month': 100,\n",
      "          'maximum_file_size': 1048576,\n",
      "          'name': 'Test',\n",
      "          'price_per_month': 0},\n",
      " 'test_credits_remaining': 100}\n"
     ]
    }
   ],
   "source": [
    "# Zamzar API authentication\n",
    "api_key = '9e485cc256a330127609b10172392af7139683df'\n",
    "auth_token = HTTPBasicAuth(api_key, '')\n",
    "\n",
    "endpoint = \"https://api.zamzar.com/v1/account\"\n",
    "res = requests.get(endpoint, auth=auth_token)\n",
    "pprint(res.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T09:48:17.156098Z",
     "start_time": "2018-10-05T09:48:10.975984Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 are already downloaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a779d788a8e94cecabf856d284f897cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=58), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 new jobs posted\n"
     ]
    }
   ],
   "source": [
    "# You need a list to store all the job_ids from the response of posting the conversion job, \n",
    "# if you are using the Zamzar API\n",
    "job_ids = []\n",
    "csv_directory = directory + 'csv/'\n",
    "already_done = set(f.partition('.')[0] for f in os.listdir(csv_directory)) \\\n",
    "               if os.path.exists(csv_directory) else set()\n",
    "print(f'{len(already_done)} are already downloaded.')\n",
    "\n",
    "for file_name in tqdm_notebook(glob.glob(directory + '*.pdf')):\n",
    "    ## Write code here to post a job, and append each job's id into job_ids ##\n",
    "    endpoint = \"https://api.zamzar.com/v1/jobs\"\n",
    "    with open(file_name, 'rb') as f:\n",
    "        just_name = file_name.partition('\\\\')[2].partition('.')[0]\n",
    "        if just_name in already_done:\n",
    "            continue\n",
    "        \n",
    "        file_content = {'source_file': f}\n",
    "        data_content = {'target_format': 'csv'}\n",
    "        res = requests.post(endpoint, \n",
    "                            data=data_content, \n",
    "                            files=file_content, \n",
    "                            auth=auth_token)\n",
    "        try:\n",
    "            job_ids.append(res.json()['id'])\n",
    "        except: \n",
    "            pprint(res.json())\n",
    "\n",
    "print(f'{len(job_ids)} new jobs posted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below this cell write the code to download the completed files. First check if a job_id's status is completed and wait until it is. After it has been completed, download the file and save it.\n",
    "\n",
    "The exact code required here is all in the documentation, the only additional task you have to do on your own is figure out a way to find out which file has just been received from the job_id, and name the local file.\n",
    "\n",
    "**Please look at the Example JSON response in the [documentation](https://developers.zamzar.com/docs) to learn how to figure out the filenames, job status etc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T09:48:29.345988Z",
     "start_time": "2018-10-05T09:48:29.335934Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(csv_directory):\n",
    "    os.mkdir(csv_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T09:51:48.674486Z",
     "start_time": "2018-10-05T09:51:42.964611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a220cc50e04e6c81923093a3223089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Job ID', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Downloading', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Your code goes here ##\n",
    "for job_id in tqdm_notebook(job_ids, desc='Job ID'):\n",
    "    endpoint = f\"https://api.zamzar.com/v1/jobs/{job_id}\"\n",
    "    res = requests.get(endpoint, auth=HTTPBasicAuth(api_key, ''))\n",
    "    \n",
    "    while res.json()['status'] != 'successful': \n",
    "        # Wait for job to be done, since jobs were posted in order\n",
    "        #   just waiting for the first job will be required. The rest \n",
    "        #   will probably be done by the time this loop exits\n",
    "        pass\n",
    "    \n",
    "    target_file = res.json()['target_files'][0]\n",
    "    file_id = target_file['id']\n",
    "    local_filename = csv_directory + target_file['name']\n",
    "\n",
    "    endpoint = f\"https://api.zamzar.com/v1/files/{file_id}/content\"\n",
    "    res = requests.get(endpoint, stream=True, auth=auth_token)\n",
    "\n",
    "    try:\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            file_iter = res.iter_content(chunk_size=1024)\n",
    "            for chunk in tqdm_notebook(file_iter, desc='Downloading', leave=False):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    f.flush()\n",
    "    except IOError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Parsing the CSV File\n",
    "Marks: 35\n",
    "\n",
    "This is perhaps the most difficult part of the assignment, you have to follow a similar strategy to what you did the Udacity Lab 1. You can not simply use Pandas read_csv since the conversion is not perfect and there will be rows with different number of columns, which Pandas does not take care of.\n",
    "\n",
    "### **Main Task:**\n",
    "* Write a function that parses a CSV into a Pandas DataFrame\n",
    "* Each DataFrame should consist of three columns with headers Bank, Donor_Name, and Amount\n",
    "* The date should be retrieved from the given filename \n",
    "* The Donor_Name can be NaN, as it is in a lot of cases. But try to retrieve as much information as possible\n",
    "* Remove all \"Page of\" rows\n",
    "* Don't include the header rows (e.g. \"SUPREME COURT FUND....\") into the DataFrame\n",
    "* The Amount should be converted into a Pandas numeric at the end\n",
    "\n",
    "### Other info:\n",
    "Some important resources for this part are (you can choose any one tutorial that you feel is easy to understand, they all cover roughly the same content):\n",
    "* [RegEx Tutorial 1](https://www.regular-expressions.info/)\n",
    "* [RegEx Tutorial 2](https://regexone.com/lesson/introduction_abcs)\n",
    "* [RegEx Tutorial 3](https://www.rexegg.com/)\n",
    "* [RegEx Cheatsheat](https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285)\n",
    "* This [RegEx Editor](https://regex101.com/) is your best friend since you can test your expression separately on this\n",
    "\n",
    "You will probably have to use the CSV reader in order to get all the rows of the file. You can learn more about it using this [tutorial](https://www.alexkras.com/how-to-read-csv-file-in-python/).\n",
    "\n",
    "Some tips:\n",
    "* First find out how many columns are in each row\n",
    "* Print out rows which are longer than they should be (they should all be of length 3)\n",
    "* Try to find patterns in how the data is spread, and what common problems exist in all rows\n",
    "* Write some regex to try an extract the amount from the problem row and then:\n",
    "    * Put the amount as the third column\n",
    "    * Merge the rest of the string as a name of the donor in the 2nd column\n",
    "* Also check if the rows with 3 columns are correctly formatted or not, many of them would probably not be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T09:53:00.793648Z",
     "start_time": "2018-10-05T09:52:38.504236Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re # To use regular expressions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "example_file = csv_directory + '17-08-2018.csv' # Assuming the file is in the folder all_csvs and is named appropriately\n",
    "# This is one of the most problematic files which is why I have included this in the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T10:13:01.723927Z",
     "start_time": "2018-10-05T10:13:01.708332Z"
    }
   },
   "outputs": [],
   "source": [
    "def parser(filename):\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        orig = [row for row in reader]\n",
    "        \n",
    "        # Convert list into string\n",
    "        data = [' '.join(row) for row in orig[4:]]\n",
    "\n",
    "        # Clean commas in the currency amounts\n",
    "        data = [line.replace(',', '') for line in data]\n",
    "\n",
    "        # Remove 'Total' and 'Page number' rows\n",
    "        data = [row for row in data if 'Total' not in row and 'Page' not in row]\n",
    "\n",
    "        records = []\n",
    "        for line in data:\n",
    "            amount = re.search(r' [1-9]\\d*', line)\n",
    "            amount = amount.group() if amount else ''\n",
    "\n",
    "            bank = re.search(r'.*(LTD|Ltd|LIMITED|Limited|Bank|BANK|BSC|A/C)', line)\n",
    "            bank = bank.group() if bank else ''\n",
    "            \n",
    "            account_num = re.search(r'[^\\d]0\\d*', line)\n",
    "            account_num = account_num.group() if account_num else ''\n",
    "\n",
    "            person = line.replace(amount, '').replace(bank, '').replace(account_num, '')\n",
    "\n",
    "            records.append((bank.strip(), person.strip(), amount.strip()))\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T18:04:14.514879Z",
     "start_time": "2018-10-04T18:04:14.436712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bank</th>\n",
       "      <th>Donor_Name</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL BARAKA BANK (PAKISTAN) LTD</td>\n",
       "      <td>MOAZZAM ALI</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL BARAKA BANK (PAKISTAN) LTD</td>\n",
       "      <td>MEHMOOD KHAN LODHI</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL BARAKA BANK (PAKISTAN) LTD</td>\n",
       "      <td>M UMAR</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AL BARAKA BANK (PAKISTAN) LTD</td>\n",
       "      <td>SAFDAR</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL BARAKA BANK (PAKISTAN) LTD</td>\n",
       "      <td>AMBREEN ZIA</td>\n",
       "      <td>1600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Bank          Donor_Name  Amount\n",
       "0  AL BARAKA BANK (PAKISTAN) LTD         MOAZZAM ALI   200.0\n",
       "1  AL BARAKA BANK (PAKISTAN) LTD  MEHMOOD KHAN LODHI   500.0\n",
       "2  AL BARAKA BANK (PAKISTAN) LTD              M UMAR  1000.0\n",
       "3  AL BARAKA BANK (PAKISTAN) LTD              SAFDAR  1000.0\n",
       "4  AL BARAKA BANK (PAKISTAN) LTD         AMBREEN ZIA  1600.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember, remove headers and convert all amounts to Numeric; if it can't be converted it needs to be NaN\n",
    "def read_csv(filename):\n",
    "    headers = ['Bank', 'Donor_Name','Amount']\n",
    "    \n",
    "    raw_data = parser(filename)\n",
    "    \n",
    "    df = pd.DataFrame.from_records(raw_data, columns=headers)\n",
    "    df['Amount'] = pd.to_numeric(df['Amount'])\n",
    "    \n",
    "    return df\n",
    "    \n",
    "read_csv(example_file).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Importing Full Dataset\n",
    "Marks: 10 \n",
    "\n",
    "The only additional task in this part is to:\n",
    "* Run the parser on all the files\n",
    "* For each file **add a 'Date' column, which should be inferred from the filename**\n",
    "* Concatenate each DataFrame into one large DataFrame. *Hint: concat*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-04T18:08:34.752916Z",
     "start_time": "2018-10-04T18:08:32.116657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ecef5abadd4514bd50b9435ef60966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=57), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                            Bank          Donor_Name  Amount\n",
      "0  AL BARAKA BANK (PAKISTAN) LTD         MOAZZAM ALI   200.0\n",
      "1  AL BARAKA BANK (PAKISTAN) LTD  MEHMOOD KHAN LODHI   500.0\n",
      "2  AL BARAKA BANK (PAKISTAN) LTD              M UMAR  1000.0\n",
      "3  AL BARAKA BANK (PAKISTAN) LTD              SAFDAR  1000.0\n",
      "4  AL BARAKA BANK (PAKISTAN) LTD         AMBREEN ZIA  1600.0\n",
      "(115482, 3)\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(csv_directory + '*.csv')\n",
    "\n",
    "full_data = pd.DataFrame()\n",
    "for file in tqdm_notebook(files):\n",
    "    df = read_csv(file)\n",
    "    # modify df by adding a Date column\n",
    "    # the date should be datetime object, not string\n",
    "    # the filename is the date\n",
    "    # Tooba will work here :*\n",
    "    \n",
    "    full_data = full_data.append(df)\n",
    "\n",
    "print(full_data.head())\n",
    "print(full_data.shape)\n",
    "# print(full_data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Integrity Checks\n",
    "Marks: 20\n",
    "\n",
    "* How many NaN values are there in each column? Why are they there? \n",
    "* What are the maximum and minimum values, is there anything peculiar about the max values?\n",
    "* Are there any rows which are not NaN but should still be a different DataFrame altogether?\n",
    "* Should these problem rows be removed? Can they be useful in other ways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
